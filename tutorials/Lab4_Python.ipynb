{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 Overview\n",
    "## First Half\n",
    "- Machine Learning Discussion.\n",
    "- What to expect for Project 2.\n",
    "\n",
    "## Second Half\n",
    "Advanced Content:\n",
    "- Introduction to PySpark continued.\n",
    "- Introduction to Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf\n",
    "- Generalize the model (train dataset will almost never be the same as \"test\" dataset, let alone real world test data)\n",
    "    - Avoid overfitting\n",
    "    - Remember the bias that can occur with a normal train-test split (80% train vs 20% test is not representitive and can change depending on the split)\n",
    "    - Cross validation\n",
    "- There is never enough data\n",
    "    - Models will always make certain assumptions \n",
    "    - Even the best model may be no better than a DummyClassifier (random predictions). Think about real world problems with 25% accuracy such as NLP...\n",
    "- Overfitting\n",
    "    - Bias: a learner's tendency to consistently learn the wrong labels\n",
    "    - Variance: tendency to learn random things irrespective of the true labels\n",
    "    ![bv](./cloud/bv.PNG)\n",
    "    - Consider bias on linear models. They rely on the data being linearly seperable by a hyperplane. But what if it is not...\n",
    "    - Consier variance on tree models.  They can represent any boolean function to seperate the data, but can learn very different things depending on the training data\n",
    "    - Can do cross validation or maybe a $\\chi^2$ test (from statistics / lsm) during feature selection \n",
    "- Curse of dimensionality\n",
    "    - As your feature space increases, the number of possible configurations grows exponentially, thus, the number of possible configurations covered by your observation decreases. \n",
    "    - Imagine a dataset with more features than observations\n",
    "    - https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e\n",
    "- Feature Engineering is Key\n",
    "    - Why do models fail? Most likely due to the feature space used.\n",
    "    - You will come to understand that all the time in ML is spent on preprocessing and feature engineering.\n",
    "    - A simple model that is interpretable and utilises all the data is far more favourable than a complex model with cannot be interpreted easily with a subset of the data...\n",
    "- Learn several models\n",
    "    - Use a baseline to figure out the difficulty of your problem\n",
    "    - Try a wide range of models (regression vs trees vs svm vs clustering) and analyse why some may do better than other\n",
    "    - Think of techniques such as bagging, boosting, etc\n",
    "        - Bagging: Generate random variations of the training set through resampling then use a set of classifiers in a voting system\n",
    "        - Boosting: Training examples have weights which are varied so each new classifier aim to focus on the examples that previous classifiers failed \n",
    "- Correlation does not imply causation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
