{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Applied Data Science (MAST30034) Tutorial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installing `pyspark` and Pre-Req Notebook (15-30 minutes)\n",
    "- Basic intro to Apache Spark (30-60 minutes)\n",
    "- Project 1 Tips (yes, it's already out and we **strongly recommend you start today**) (remainder of time)\n",
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Larger Datasets with a Scalable Solution!\n",
    "Consider the size of the datasets you have worked with at Uni. Probably a few hundred megabytes or a couple gigabytes. Whilst `pandas` and `Excel` do have their use cases, it is not feasible to use them when you work with larger datasets over several gigabytes.\n",
    "\n",
    "For example:\n",
    "1. 20k rows would be hard for Excel, but easy for `pandas`.\n",
    "2. A few million records would be doable for `pandas` depending on RAM (let's say 16GB or 32GB to be generous).\n",
    "3. Now, consider 100 million rows over several gigabytes. `pandas` **is not your solution**.\n",
    "\n",
    "Why?\n",
    "\n",
    "`pandas` works in-memory. That is, you are limited by RAM which can be hard to come across for the average person. Even with 32GB or 64GB memory, it is best to use Apache Spark, which is designed to work with large datasets.\n",
    "\n",
    "![image.png](https://spark.apache.org/images/spark-logo-trademark.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "- Windows 10 or 11 users are required to install `WSL` or `WSL2` for `pyspark`. This is something that you should take the time to learn how to use and install now for a future career in the tech industry. If you have yet to install it, please visit https://github.com/akiratwang/COMP20003\n",
    "- MacOS (Intel) or Linux is all good. If you are using an M1 or M2 chip, you will need to follow some specific instructions.\n",
    "\n",
    "\n",
    "If you really don't want to use spark, you can use `pandas` as usual. Just be aware of memory limitations and missed opportunity for upskilling yourself for a job! For those who don't want to use Spark, please run `pip3 install pandas fastparquet` and you can just read it in like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:52.133940Z",
     "start_time": "2022-07-18T07:52:51.731431Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../../data/tlc_data/2022-01.parquet')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then follow the tutorial using the alternative `pandas` syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "0. (Pre-Req) Install WSL2 for Windows 10 users. MacOS users, please ensure your terminal is set to `bash`.\n",
    "1. We recommend a fresh environment for this subject as there can be package conflicts, but all good if you are lazy. If you are getting errors, please refresh your environment before coming to us for help.\n",
    "2. Install `Java` and `PySpark`:  \n",
    "    - Linux (WSL or WSL2 or native)\n",
    "        ```bash\n",
    "        # Update apt formula\n",
    "        sudo apt update\n",
    "        # install java\n",
    "        sudo apt install openjdk-8-jdk -y\n",
    "        # add to path\n",
    "        echo 'JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"' | sudo tee -a /etc/environment\n",
    "        # apply to environment\n",
    "        source /etc/environment\n",
    "        # install spark\n",
    "        pip3 install pyspark pyarrow pandas\n",
    "        ```\n",
    "    - MacOS\n",
    "        1. Install [Homebrew](https://brew.sh/)\n",
    "            - If your shell prompts to set `zsh` as default shell with `chsh -s /bin/zsh`, run that first!!\n",
    "        2. Install/setup Java/JAVA_HOME (spark uses java for backend)\n",
    "            ```\n",
    "            # For Intel CPU\n",
    "            # install java 8 and link to system java wrapper\n",
    "            brew install openjdk@8 \n",
    "            # For newer version of brew, try the command below if brew install doesn't work\n",
    "            #brew install --cask homebrew/cask-versions/adoptopenjdk8\n",
    "            sudo ln -sfn /usr/local/opt/openjdk@8/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-8.jdk\n",
    "            # add to path (earlier OSX defaults to bash while newer ones defaults to zsh)\n",
    "            echo 'export JAVA_HOME=\"$(/usr/libexec/java_home -v1.8)\"' | tee -a $HOME/.bashrc $HOME/.zshrc\n",
    "            ```\n",
    "            (If you are using MacOS (M1 or M2 chip), follow [this guide](https://code2care.org/q/install-native-java-jdk-jre-on-apple-silicon-m1-mac) for Java JDK.)\n",
    "        3. Install python packages/spark\n",
    "            ```bash\n",
    "            # reload java path\n",
    "            source $HOME/.bashrc ; source $HOME/.zshrc\n",
    "            # install spark. Note: if you are using anaconda/conda environments, you need to make sure the pip3 is the correct pip3!\n",
    "            # Or you should install with conda directly!\n",
    "            #conda install pyspark pyarrow pandas\n",
    "            pip3 install pyspark pyarrow pandas\n",
    "            ```\n",
    "3. If you are using Vscode on windows, you also need to follow this [instruction](https://code.visualstudio.com/docs/remote/wsl-tutorial).\n",
    "4. After you installed everything, reload your notebook kernel.\n",
    "\n",
    "Run the code below to see if you have installed it. As long as it runs (despite red warnings) and there are no errors, you're ready to go!\n",
    "\n",
    "Touble shooting guides:\n",
    "![ELPERS](https://cdn.betterttv.net/emote/5ec42fc6c752192ee9603b94/1x)\n",
    "1. Help! The module is still not found after I installed everything!\n",
    "    - run `which pip` `which python` in your terminal and compare that with results of `import sys; sys.executable` running from your jupyter notebook. They have to be the same path (why?).\n",
    "    - If not same path, change the kernel of your jupyter notebook to using that python kernel.\n",
    "2. HELP!! The java instance stopped when executing the cell\n",
    "    - Ensure java is installed (commands executes without error)\n",
    "    - make sure `echo $JAVA_HOME` produces the proper location (i.e. it points at where your java is installed)\n",
    "3. hELppp! `conda`, `pip`, `apt`, `brew`... not found!\n",
    "    - Install the required softwares and make sure their home folder are present in your `echo $PATH`\n",
    "4. elPppppp! I am using windows and I don't want to use WSL\n",
    "    - NO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Phikho\\\\anaconda3\\\\envs\\\\MAST30034\\\\python.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:12.646667Z",
     "start_time": "2022-07-18T07:52:52.135234Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Spark Session\n",
    "To begin with Spark, we need to start a `SparkSession` class (see above).\n",
    "- `appName`: Name of the Spark app\n",
    "- `config`: Configurations to initialise with. We will initialise this example with `'spark.sql.repl.eagerEval.enabled'` which enables a nicer HTML display (similar to `pandas`) for the DataFrame outputs.\n",
    "- `.getOrCreate()`: Create the spark session.\n",
    "    \n",
    "A general note is to understand that Spark is **immutable**. We'll discuss it further down the track, but for now, just remember this!\n",
    "\n",
    "Documentation is also going to be your saving grace. If you have tried your best **and have read the documentation and researched on Stack Overflow** but still can't get it working, then you can ask your tutor for help. This is how it works in Industry.\n",
    "\n",
    "## Reading in the Parquet\n",
    "As of 2022, TLC has made a **great decision** to drop `csv` and adopt `parquet` formats instead. So, what's a `parquet`? \n",
    "\n",
    "Related materials:\n",
    "1. [What if you could get the simplicity, convenience, interoperability, and storage niceties of an old-fashioned CSV with the speed of a NoSQL database and the storage requirements of a gzipped file? Enter Parquet.](https://databricks.com/session/spark-parquet-in-depth)\n",
    "2. [The Parquet Format and Performance Optimization Opportunities](https://databricks.com/session_eu19/the-parquet-format-and-performance-optimization-opportunities)\n",
    "\n",
    "CSV:\n",
    "- `csv` are tabular data formats read in line by line using a `,` delimiter.\n",
    "- That is, these are stored by rows.\n",
    "- They consume a lot of disk space and are one of the **most inefficient** ways of storing data.\n",
    "- However, they are widely used and easy to use for smaller datasets.\n",
    "\n",
    "Parquet:\n",
    "- `parquet` on the other hand is stored in columns and (ELI5) are very efficient with data formats.\n",
    "- For example, a single row in a `csv` can contain several different data types. \n",
    "- `parquet` just have the single data type per column, allowing compression algorithms to be applied to reduce disk space and read efficiency.\n",
    "- For alternatives to `csv` for row based data formats, you can take a look at `avro`.\n",
    "\n",
    "![Divisions of storage format](../../media/storageformat.png)\n",
    "\n",
    "Cost Analysis from Amazon Web Services (AWS): ![image.png](https://miro.medium.com/max/1400/1*vdasMxTjInhBXIRA8K1XYQ.png)\n",
    "\n",
    "Spark Docs\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=read%20parquet\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html?highlight=show#pyspark.sql.DataFrame.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:21.468401Z",
     "start_time": "2022-07-18T07:53:12.650400Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/C:/Users/Phikho/Desktop/Applied Data Science/MAST30034_Python/data/tlc_data/2022-01.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# sdf = spark df = spark data frame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/tlc_data/2022-01.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m sdf\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m, vertical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MAST30034\\lib\\site-packages\\pyspark\\sql\\readwriter.py:458\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    453\u001b[0m recursiveFileLookup \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursiveFileLookup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema, pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m    455\u001b[0m                recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[38;5;241m=\u001b[39mmodifiedBefore,\n\u001b[0;32m    456\u001b[0m                modifiedAfter\u001b[38;5;241m=\u001b[39mmodifiedAfter)\n\u001b[1;32m--> 458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MAST30034\\lib\\site-packages\\py4j\\java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MAST30034\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/C:/Users/Phikho/Desktop/Applied Data Science/MAST30034_Python/data/tlc_data/2022-01.parquet"
     ]
    }
   ],
   "source": [
    "# sdf = spark df = spark data frame\n",
    "sdf = spark.read.parquet('../../data/tlc_data/2022-01.parquet')\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark UI is quite ugly at times, so if you miss `pandas` and want the \"nice\" display you can set `spark.sql.repl.eagerEval.enabled` to `True` in the config. To see the nice format, use `.limit()`.\n",
    "\n",
    "`pyspark`'s `.show()`, `.head()`, `.limit()`, etc are all alternatives to `pandas`'s `.head()` (`.tail` exists in both `pandas` and `pyspark`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.610330Z",
     "start_time": "2022-07-18T07:53:21.470526Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has also been designed to read in directories as well! We won't be using it for the tutorial, but if you wish to use it for your project, feel free to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.962252Z",
     "start_time": "2022-07-18T07:53:24.612542Z"
    }
   },
   "outputs": [],
   "source": [
    "# here, we give it the directory, rather than a specific parquet\n",
    "sdf_all = spark.read.parquet('../../data/tlc_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the number of records, use the `.count()` method. The equivalent in `pandas` would be `len(df)` or `df.shape` or alternative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:26.305371Z",
     "start_time": "2022-07-18T07:53:24.963523Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.count(), sdf_all.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the data types of our `sdf`, we can use:\n",
    "- `.printSchema()` to print it nicely.\n",
    "- `.schema` for the actual schema object\n",
    "\n",
    "The `pandas` alternative is `df.dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:26.320582Z",
     "start_time": "2022-07-18T07:53:26.306928Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:26.331218Z",
     "start_time": "2022-07-18T07:53:26.322442Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for the available data types: https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations\n",
    "### Selection\n",
    "To show a specific column, we will use `sdf.select(col).limit(5)`. \n",
    "- The equivalent in `pandas` is `df[col].head()`.\n",
    "\n",
    "To _access_ a specific column, use the `sdf[col]` syntax (equivalent to `df[col]`). Avoid using `sdf.col` or `df.col` as it is **not** robust (cannot handle columns with spaces) or future-proof. \n",
    "\n",
    "For multiple columns, pass them through an array as usual.\n",
    "\n",
    "Please note, this selection is only good for seeing bits and pieces of data and not for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:27.797233Z",
     "start_time": "2022-07-18T07:53:26.334596Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.select('passenger_count').limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Students to write code to select the first 10 records for `passenger_count` and `trip_distance`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:28.900795Z",
     "start_time": "2022-07-18T07:53:27.798257Z"
    }
   },
   "outputs": [],
   "source": [
    "# write code here to select the first 10 records for `passenger_count` and `trip_distance`\n",
    "sdf.select([\"passenger_count\", \"trip_distance\"]).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "For filtering data, we use `sdf.filter(condition)` or `sdf.where(condition)` (they are aliases of each other)\n",
    "- The equivalent in `pandas` is `df.loc[condition].head()`\n",
    "- When using multiple conditions, use parenthesis and `&` (AND) / `|` (OR)\n",
    "\n",
    "To do so, we will use `pyspark.sql.functions.col` to specify the column we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:28.906507Z",
     "start_time": "2022-07-18T07:53:28.901795Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:29.423230Z",
     "start_time": "2022-07-18T07:53:28.907820Z"
    }
   },
   "outputs": [],
   "source": [
    "F.col(\"passenger_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is just a \"column type\" and doesn't do much. We'll come back to this in the next tutorial. For now, take our word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:32.297305Z",
     "start_time": "2022-07-18T07:53:29.426307Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.filter(F.col('passenger_count') == 5).limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Students to write code to retrieve all non-zero passenger counts and all non-zero trip distances using `.where()`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:34.405557Z",
     "start_time": "2022-07-18T07:53:32.301980Z"
    }
   },
   "outputs": [],
   "source": [
    "# write code here to retrieve all non-zero passenger counts and all non-zero trip distances using where\n",
    "sdf.where((F.col('passenger_count') > 0) & (F.col('trip_distance') > 0)).limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy (Aggregation)\n",
    "To groupby the data (i.e mean), we can use `sdf.groupby(col).mean(aggregated columns).limit(5)`\n",
    "- The equivalent in `pandas` is `df.groupby(col)[aggregated columns].mean().head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:37.585505Z",
     "start_time": "2022-07-18T07:53:34.406498Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.groupby('passenger_count').mean('trip_distance').limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply multiple different aggregations and change their output names using `.agg()` and `.alias()`! To see the list of all SQL functions, visit https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "\n",
    "We'll also use `.orderBy()` to display the results nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:39.152137Z",
     "start_time": "2022-07-18T07:53:37.586357Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "aggregated_results = sdf \\\n",
    "                    .groupBy(\"passenger_count\") \\\n",
    "                    .agg(\n",
    "                        F.mean(\"total_amount\").alias(\"avg_trip_amount_usd\"),\n",
    "                        F.max(\"trip_distance\").alias(\"max_trip_distance_miles\")\n",
    "                    ) \\\n",
    "                    .orderBy(\"passenger_count\")\n",
    "\n",
    "aggregated_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "By default, Spark will save your data sources as a `parquet` (highly recommended). If you wish to take a smaller sample and save it as a `csv` to load into `pandas`, that is also fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:54:51.894294Z",
     "start_time": "2022-07-18T07:54:49.716378Z"
    }
   },
   "outputs": [],
   "source": [
    "aggregated_results.write.mode('overwrite').parquet('../../data/tute_data/aggregated_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your directory may look a bit funky like this:\n",
    "\n",
    "![image.png](../../media/aggregated_results_dir.png)\n",
    "\n",
    "Don't worry, just leave it as is (we don't have time to cover everything about Spark unfortunately) and you can just read in the directory as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:54:53.082989Z",
     "start_time": "2022-07-18T07:54:52.455856Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_results = spark.read.parquet('../../data/tute_data/aggregated_results')\n",
    "temp_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary (and Break)\n",
    "Cool, we've covered the very very basics of Spark and will now cover the basics of plotting.\n",
    "\n",
    "Rest assured, we will cover more intricate transformations for the next tutorial (which you may go ahead in of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Data for Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst Spark is amazing at handling big data sets, it isn't a great idea to plot all of it. We suggest taking a maximum of 5% of records for the tutorial. \n",
    "\n",
    "You can up it to your requirements, but we recommend sticking to less than 1 million records per month for visualization purposes.\n",
    "\n",
    "**Project 1 Checklist:**\n",
    "- You have justified your sample size (i.e due to runtime, distribution of data, etc)\n",
    "- You have justified your sampling method (i.e random, stratified, etc)\n",
    "- You have detailed in your report that you have sampled for visualization purposes BUT your analysis still uses the full distribution of data\n",
    "- You mention any issues that can potentially be caused by sampling (i.e biased visualisation if using random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T05:45:28.804443Z",
     "start_time": "2022-07-12T05:45:28.724408Z"
    }
   },
   "source": [
    "To sample your data and convert it into a `pandas` dataframe, you can use the `.toPandas()` and save a sample of the `sdf` to read it in. We will also fix the random seed to be `0` just for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:54:55.323936Z",
     "start_time": "2022-07-18T07:54:55.315826Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:03.216732Z",
     "start_time": "2022-07-18T07:54:55.485179Z"
    }
   },
   "outputs": [],
   "source": [
    "df = sdf.sample(SAMPLE_SIZE, seed=0).toPandas()\n",
    "df.to_csv('../../data/tute_data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:03.268486Z",
     "start_time": "2022-07-18T07:55:03.218004Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_parquet('../../data/tute_data/sample_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just spend a moment and look at the disk space the `csv` takes for the 5% sample size (13.3mb). Compare that to the `parquet` which isn't even 3mb, let alone the full sample size in `parquet` format taking only 37mb of disk space.\n",
    "\n",
    "Let that sink in and give our thanks to the devs who made Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:03.402341Z",
     "start_time": "2022-07-18T07:55:03.269245Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_csv = pd.read_csv('../../data/tute_data/sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:03.417770Z",
     "start_time": "2022-07-18T07:55:03.403812Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_parquet = pd.read_parquet('../../data/tute_data/sample_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you save every dataframe or aggregation as `parquet` so you don't keep running your notebook from top to bottom waiting 20 years for a result, or have so many variables and dataframes defined that you run out of memory for small transformations.\n",
    "\n",
    "We strongly suggest you have a `code` folder in your Project 1 directory with the following structure:\n",
    "- `preprocessing_notebook_part_1.ipynb`: outputs a structured parquet format and saves it.\n",
    "- `preprocessing_notebook_part_2.ipynb`: reads in the output from above and does some aggregations and sampling before saving it.\n",
    "- `data_analysis_xyz.ipynb`: conducts analysis on a single sample or aggregation from the output above.\n",
    "- `data_analysis_abc.ipynb`: conducts analysis on another single sample or aggregation from the output above.\n",
    "- `...`\n",
    "\n",
    "This is a very basic version of what you call a \"data pipeline\" (or ETL pipeline, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Tips and Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT PLEASE READ THIS\n",
    "First and foremost, you want to be familiar with the homepage https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "Read through the relevant data dictionaries:\n",
    "- **MUST READ:** https://www1.nyc.gov/assets/tlc/downloads/pdf/trip_record_user_guide.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_fhv.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf\n",
    "\n",
    "Why? Your tutors can be treated as \"experts\" in this field. To prepare you for the Industry Project, we need to assess students on adhering to requirements and business rules. \n",
    "\n",
    "The tutor team should know this dataset inside out. If you are incorrectly filtering records without sufficient justification, you will be losing marks as per requirements.\n",
    "\n",
    "### An Incorrect Example\n",
    "- Scenario: Student does analysis on `tip_amount` and finds several `NULL` values and either drops them or includes it in the analysis. Later on, they use a regression model to predict this value.\n",
    "\n",
    "- Result: According to the data dictionary, `tip_amount` is automatically populated for credit card tips (`payment_type` is `1`). Cash tips are not included. This means that the students' analysis included all payment types despite this field clearly specifying the rule. \n",
    "\n",
    "- Penalty: The student will lose marks on the analysis section. The modelling section will be marked _assuming_ they got this filtering method correct. However, if another issue pops up due to this, there will be another penalty applied. Please get this right!\n",
    "\n",
    "- Solution: Student should filter for only `payment_type=1` and now, the student can (hopefully) conduct correct analysis on `tip_amount`.\n",
    "\n",
    "Several students over the past few years have lost many marks for simple rules like this (especially `tip_amount`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readable Code\n",
    "- We will be assessing the quality of your code and how you present it in your notebooks. \n",
    "- This is because there is no point writing code that cannot be easily interpreted. At the end of the day, employers and clients are not only paying for your analysis, but also the corresponding code. \n",
    "- If your code is confusing or difficult to read, there is little chance your client will come back to you.\n",
    "\n",
    "**Variable Names:**  \n",
    "As long as you are consistent, then it is fine. For example, commit to either using:\n",
    "- Snake Case: words are seperated by underscores such as `variable_name`\n",
    "- Camel Case: words are seperated by captials such as `variableName`\n",
    "\n",
    "Your variables should be contextual and describe the code. That is, try to name your variables to be understandable **without comments**.\n",
    "\n",
    "**Comments and Docstrings (w.r.t JupyterNotebook Cells):**  \n",
    "Cells in Jupyter Notebook should aim to do one \"block of logic\" at a time (i.e importing libraries, defining functions, filtering rows, etc).\n",
    "- If it takes a reader more than a few seconds to understand your cell, you need comments.\n",
    "- Your functions need to have docstrings describing what they do. If you forgot, search it online or go visit your COMP10001 Grok course.\n",
    "- Use markdown cells for longer comments or explaining logic, inline comments in code for short descriptions of hard-to-understand code.\n",
    "\n",
    "We won't ask you to run `flake8` or `pylint` on your notebooks. We just ask for good comments in the code and markdown cells, reasonable variable names, and clean directories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1c6f7b18ea35922dad1f927d5d0123541ee5478d7a9729c6a2c6ed680be427a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
