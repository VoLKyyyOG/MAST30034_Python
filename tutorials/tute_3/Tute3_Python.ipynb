{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Overview\n",
    "\n",
    "Reminder that Project 1 is due this Friday.\n",
    "\n",
    "## First Half\n",
    "- Linear Regression.\n",
    "- Evaluation Metrics.\n",
    "- Penalized Regression (LASSO and Ridge).\n",
    "- Generalised Linear Models (GLM).\n",
    "- Discussion.\n",
    "\n",
    "## Second Half\n",
    "Revision:\n",
    "- Any code related questions for Python.\n",
    "- (Windows 10 Users) Installing WSL2 (Ubuntu 20.04) for a clean environment.\n",
    "\n",
    "Advanced Content:\n",
    "- Introduction to `PySpark` continued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:26:21.417929Z",
     "start_time": "2021-08-10T02:26:20.341260Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from statsmodels.formula.api import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:26:44.293880Z",
     "start_time": "2021-08-10T02:26:44.100060Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/lab_specific/100k_yellow_2015_05.csv\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's try to predict `total_amount` using `fare_amount, tip_amount, toll_amount, trip_distance, VendorID` as predictors.\n",
    "\n",
    "Some things to take note:\n",
    "- `tip_amount` is only valid for `payment_type == 1` (card)\n",
    "- `VendorID` is categorical, with only two possible values (`1` or `2`) so we should make it boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:26:46.995501Z",
     "start_time": "2021-08-10T02:26:46.966030Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter dataframe\n",
    "COL_FILTER = ['total_amount', 'fare_amount', 'tip_amount', 'tolls_amount', 'trip_distance', 'VendorID']\n",
    "df_filtered = df.loc[df['payment_type'] == 1, COL_FILTER].reset_index(drop=True)\n",
    "\n",
    "# same as df_filtered['VendorID'].astype(bool)\n",
    "df_filtered['VendorID'] = df_filtered['VendorID'] == 1 \n",
    "\n",
    "df_filtered.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are looking for linear relationships between our chosen response `total_amount`.   \n",
    "- Now I'm not sure what kind of life you've lived, but I'm fairly certain that we can infer that `total_amount` will have a positive linear relationship with `fare_amount`. Let's see a quick plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:26:49.147614Z",
     "start_time": "2021-08-10T02:26:48.975581Z"
    }
   },
   "outputs": [],
   "source": [
    "df_filtered[['total_amount', 'fare_amount']].plot.scatter(x='fare_amount', y='total_amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, obviously this looks like an overall positive linear relationship.\n",
    "- How might we statistically test this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R, we would do something like this for (Ordinary) Least Squares:\n",
    "```R\n",
    ">>> fit <- lm(total_amount~fare_amount + tip_amount + tolls_amount + trip_distance + VendorID ,data=dat_fit)\n",
    ">>> summary(fit)\n",
    "```\n",
    "```\n",
    "Call:\n",
    "lm(formula = total_amount ~ fare_amount + tip_amount + tolls_amount +\n",
    "trip_distance + VendorID, data = dat_fit)\n",
    "\n",
    "Residuals:\n",
    "Min     1Q      Median  3Q     Max\n",
    "-1.4727 -0.3295 -0.1528 0.1747 1.7975\n",
    "\n",
    "Coefficients:\n",
    "               Estimate Std. Error t value Pr(>|t|)\n",
    "(Intercept)    1.162154   0.002986 389.194  <2e-16 ***\n",
    "fare_amount    0.993388   0.000315 3153.943 <2e-16 ***\n",
    "tip_amount     1.006511   0.000826 1218.553 <2e-16 ***\n",
    "tolls_amount   0.979325   0.001285 762.428  <2e-16 ***\n",
    "trip_distance  0.011742   0.000963 12.194   <2e-16 ***\n",
    "VendorIDTRUE  -0.003125   0.002914 -1.073    0.283\n",
    "---\n",
    "Signif. codes:\n",
    "0 ^a˘A¨Y***^a˘A´Z 0.001 ^a˘A¨Y**^a˘A´Z 0.01 ^a˘A¨Y*^a˘A´Z 0.05 ^a˘A¨Y.^a˘A´Z 0.1 ^a˘A¨Y ^a˘A´Z 1\n",
    "\n",
    "Residual standard error: 0.362 on 61886 degrees of freedom\n",
    "Multiple R-squared: 0.9994,          Adjusted R-squared: 0.9994\n",
    "F-statistic: 1.953e+07 on 5 and 61886 DF, p-value: < 2.2e-16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, whatever you can do in R can also done in Python (to an extent).  \n",
    "Documentation Source: https://www.statsmodels.org/dev/generated/statsmodels.formula.api.ols.html?highlight=ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:29:25.720329Z",
     "start_time": "2021-08-10T02:29:25.692799Z"
    }
   },
   "outputs": [],
   "source": [
    "fit = ols(formula=\"total_amount ~ fare_amount + tip_amount + tolls_amount + trip_distance + VendorID\",\n",
    "         data=df_filtered).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:29:26.030645Z",
     "start_time": "2021-08-10T02:29:26.021887Z"
    }
   },
   "outputs": [],
   "source": [
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The table structure is a bit different, though it is identical in value with R's output.  \n",
    "- The coefficient table is the same, but now includes a 95% CI for the beta coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- Is this model good?\n",
    "    - The $R^2$ value is 0.999 which is insanely large. As a rule of thumb, large $R^2$ values indicate a good fit. \n",
    "    - *Perhaps too good of a fit...*\n",
    "    - AIC itself isn't important, however, if we compare it to another model (let's say an alternative model with different features chosen)...\n",
    "    - If we have a hypothesis for a null model ($\\beta=0$) vs our fitted model ($\\beta\\neq0$), then we can look at the `F-statistc = 1.953e+07`. \n",
    "    - The corresponding p-value of  this F statistic is `0.00`, which is less than $\\alpha=0.05$, so we can conclude that our fitted model is better than a null model. \n",
    "    - In other words, we reject the null hypothesis and conclude that we believe the intercept parameters to be non-zero.\n",
    "    \n",
    "    \n",
    "- How might we improve this model?\n",
    "    - If we look at the parameters, we may wish to exclude `VendorID[T.True]` as it is not significant with p-value `0.283 > 0.05`. Perhaps we should drop this attribute and fit another model without it.\n",
    "    - Additionally, we can do some feature engineering (run a decision tree and look at the splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:30:49.564721Z",
     "start_time": "2021-08-10T02:30:49.538223Z"
    }
   },
   "outputs": [],
   "source": [
    "fitter = ols(formula=\"total_amount ~ fare_amount + tip_amount + tolls_amount + trip_distance\",\n",
    "         data=df_filtered).fit()\n",
    "print(fitter.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have to values of AIC to compare with, which one is better...?\n",
    "- Well, we see a small decrease in AIC and a large decrease in BIC. Hence, we can say that the model without `VendorID` is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:30:51.354558Z",
     "start_time": "2021-08-10T02:30:51.351370Z"
    }
   },
   "outputs": [],
   "source": [
    "[fit.aic, fitter.aic], [fit.bic, fitter.bic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Regression\n",
    "- LASSO (l1) and Ridge (l2) Regression\n",
    "\n",
    "Revise in your own time if you've forgotten (this was covered in MAST30025):\n",
    "- Lecture 4 (variable selection)\n",
    "- LSM topic 5 (`ch05_handout`) slide 141/141\n",
    "- An excellent explanation on Ridge / LASSO: https://www.youtube.com/watch?v=9LNpiiKCQUo (recommended at x1.25 speed)\n",
    "\n",
    "Things you might have forgotten when working with penalized models:\n",
    "- Always good to standardize your data prior to train and test. Most models perform poorly if not standardized prior. \n",
    "- Do not fit your standardizer to test, only to train. You should transform both your train and test though.\n",
    "\n",
    "### LASSO ($\\ell_1$)\n",
    "Quick overview:\n",
    "- LASSO may cause coefficients to be set to 0 by constraining the model.\n",
    "- This is because we put a constraint where the sum of the absolute values of the coefficients must be less than some fixed value. \n",
    "- As such, some coefficients may end up having 0 which is the same as *dropping* the attribute from the model. Notably, features that are collinear (correlated) will result in one of them being reduced to 0 coefficient.\n",
    "- In this sense, it's quite similar to feature selection as you end up with a model that is much more simpler. \n",
    "- However, LASSO does not do well when the feature space is small as you may end up with an over-simplified model, as well as cases where all the features are significant or when coefficients are extremely large. \n",
    "- This is why you might want to standardize your dataset prior to fitting.\n",
    "\n",
    "Solution:\n",
    "- Requires an iterative method to solve $(\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\lambda I \\beta$\n",
    "- The $\\ell_1$ (vector normal) comes from the penalty term $\\lambda I \\beta$. Our $\\beta$ term is to the power of 1, hence we have $\\ell_1$.\n",
    "\n",
    "### Ridge ($\\ell_2$)\n",
    "Quick overview:\n",
    "- Also know as the MAP (Maximum a posteriori) estimation.\n",
    "- Aims to lower the scale of the coefficients to avoid overfitting, but does not result in coefficients being 0.\n",
    "- In contrast to LASSO, we put a constrain using the sum of squares that must be lest than a fixed value. \n",
    "- As you might guess, this means we still have several features making it less interpretable than LASSO.\n",
    "- However, Ridge Regression performs best in cases where there may be high multi-colinearity (i.e dependencies between attributes) or high linear correlation between certain attributes,\n",
    "- This is because it reduces variance in exchange for some more bias (consider variance-bias tradeoff).\n",
    "- You must also ensure that we have more observations than attributes (`n > p`) as this penalty method does not drop features, leading to worse predictions. \n",
    "\n",
    "Solution:\n",
    "- Closed-form which can be found by minimising $(\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\lambda I \\beta^T\\beta$\n",
    "- The $\\ell_2$ (vector normal) comes from the penalty term $\\lambda I \\beta^T\\beta$. Since the $\\beta$ term is squared, we have $\\ell_2$.\n",
    "- Good to practice and show that minimising the equation above yields the \\\n",
    "\n",
    "### Elastic Net\n",
    "Quick overview:\n",
    "- Combines both Ridge and LASSO into a single model utilising an $\\alpha$ parameter, where $\\alpha=0$ is Ridge and $\\alpha=1$ is LASSO.\n",
    "- Implementation Documentation: https://github.com/civisanalytics/python-glmnet/blob/master/glmnet/linear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:31:10.760181Z",
     "start_time": "2021-08-10T02:31:10.753171Z"
    }
   },
   "outputs": [],
   "source": [
    "yCOLS = ['total_amount']\n",
    "xCOLS = ['fare_amount', 'tip_amount', 'tolls_amount', 'trip_distance', 'VendorID']\n",
    "\n",
    "# standardize (by calculating the zscore) so our data has mean 0 and var 1\n",
    "# alternatively, you can use sklearn's StandardScalar\n",
    "\n",
    "from scipy.stats import zscore\n",
    "df_standard = df_filtered[xCOLS].astype(float).apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:31:13.321273Z",
     "start_time": "2021-08-10T02:31:13.299531Z"
    }
   },
   "outputs": [],
   "source": [
    "# format output to 4 decimal places\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "df_standard.describe().loc[['mean','std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `df_standard` has  $\\mu=0, \\sigma=1(=\\sigma^2)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:31:15.849478Z",
     "start_time": "2021-08-10T02:31:15.553248Z"
    }
   },
   "outputs": [],
   "source": [
    "from glmnet import ElasticNet\n",
    "\n",
    "lasso_fit = ElasticNet(alpha=1)\n",
    "lasso_fit.fit(df_standard.values, df_filtered[yCOLS].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to look at the shrinking parameter $\\lambda$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:31:29.311573Z",
     "start_time": "2021-08-10T02:31:29.308913Z"
    }
   },
   "outputs": [],
   "source": [
    "# this can be accessed using the .lambda_best_ method after fitting!\n",
    "print(f'Best lambda value for LASSO: {lasso_fit.lambda_best_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ is computed by using cross validation (iterative approach)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about our coefficients?\n",
    "- https://github.com/civisanalytics/python-glmnet/blob/master/glmnet/linear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:31:43.671569Z",
     "start_time": "2021-08-10T02:31:43.665533Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(index = ['Intercept'] + xCOLS, \n",
    "             data= [lasso_fit.intercept_] + list(lasso_fit.coef_), \n",
    "             columns = ['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `trip_distance` and `VendorID` have *shrunk* to 0. You can use `lasso_fit.predict(x)` to the predict a new set of observations by passing through the `x` matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a GLM (Optional)\n",
    "- Well, this is exactly what some of you will be learning in MAST30027 right now.\n",
    "\n",
    "Let's go through an example:\n",
    "- The `passenger_count` attribute is discrete and non-negative. If we were to predict it, a linear model will not be sufficient. \n",
    "- We know that a Poisson distribution takes in non-negative integer values, so we can use the Poisson family of GLMs to model this. \n",
    "- We will use `total_amount, trip_distance, VendorID` as our regressors.\n",
    "\n",
    "For those of you not taking MAST30027 (ELI5):\n",
    "- GLM's allow us to express relationships in a linear and additive way like normal linear regression.\n",
    "- However, it might be the case that the underlying true relationship is neither linear nor additive. \n",
    "- The transformation is done through a *link function* (in this case, Poisson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:31:55.296787Z",
     "start_time": "2021-08-10T02:31:54.945487Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import families\n",
    "\n",
    "# convert VendorID to categorical\n",
    "df['VendorID'] = df['VendorID'] == 1\n",
    "\n",
    "fit = glm(formula=\"passenger_count ~ total_amount + trip_distance + VendorID\",\n",
    "         data=df, family=families.Poisson()).fit()\n",
    "\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that `total_amount` is insignificant (`p-val=0.124>0.05`)\n",
    "- Conclude that the total fare amount does not really affect the number of passengers in a trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "- What is the Bias-Variance tradeoff with respect to linear models:\n",
    "    - Less parameters = less variance but more bias\n",
    "    - More parameters = more variance but less bias\n",
    "    - The goal depends on the problem, but generally we want an even variance and bias (intersection).\n",
    "\n",
    "\n",
    "- Is using regression on X attribute / specific dataset even a good choice...?\n",
    "    - The answer is yes, it is a good choice *to try*\n",
    "    - BUT also try other methods...\n",
    "    \n",
    "    \n",
    "- What are the pros and cons of stepwise regression?\n",
    "    - Forward Selection (start from nothing and end until significant)\n",
    "    - Backward Elimination (start with everything and end until no more can be removed)\n",
    "    - Not always the best results...\n",
    "    \n",
    "    \n",
    "- What is best subset regression and the pros and cons of it?\n",
    "    - A brute-force like method of fitting *all possible regressions* or *all possible models*\n",
    "    - Unlike stepwise, this method fits all possible models based on the variables specified, so you will get the best model possible\n",
    "    ![test](https://i.kym-cdn.com/photos/images/newsfeed/001/718/138/147.jpg)\n",
    "    \n",
    "    \n",
    "    \n",
    "- What is an assumption we make when we fit linear regression models?\n",
    "    - Well, the data has to be linearly separable. \n",
    "    - Does this also apply to other models too...? (Recall SVM and kernel function which we can use)\n",
    "    - Perhaps another model might suit the dataset... (Trees, Neural Networks, Clustering, etc...)\n",
    "    \n",
    "    \n",
    "- If you were to use a decision tree, how would you compare between two different fits? \n",
    "    - Look at Gini Impurity (probability of an incorrectly classified instance)\n",
    "    \n",
    "\n",
    "- How about baselines or other predictive machine learning models?\n",
    "    - Precision, Recall, Classification Accuracy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Requisite Tasks for the Apache Spark tutorial\n",
    "\n",
    "## WSL Environment for Windows 10\n",
    "Refer to this guide to get a native Linux terminal in Windows 10:\n",
    "- https://github.com/akiratwang/COMP20003-Setting-Up\n",
    "- Ignore all the `C` related parts, just get Ubuntu installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8684bbbdedfc9b42b999b1d31697a8d5e570e18b51cdd65265af4b894fa44cb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
