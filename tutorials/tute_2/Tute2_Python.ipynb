{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Science (MAST30034) Tutorial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyspark` (15-45 minutes):\n",
    "- _Under the hood - lazy evaluation_\n",
    "- Basic transformation functions\n",
    "- Spark SQL (Optional)\n",
    "\n",
    "`geopandas` (30 minutes):\n",
    "- Installation\n",
    "- Shapefiles\n",
    "- Spark User Defined Functions (UDF)\n",
    "\n",
    "New Visualizations and Basic Analysis (15 minutes):\n",
    "- `folium`\n",
    "- Choropleths\n",
    "- A revision of Pearson Correlation\n",
    "- Feature Engineering\n",
    "\n",
    "You may use any visualization taught in previous subjects for your Project (assumed knowledge).\n",
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:13.711878Z",
     "start_time": "2022-07-18T07:52:53.489183Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:18.864821Z",
     "start_time": "2022-07-18T07:53:13.713446Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../../data/tlc_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Immutability\n",
    "Transformations in PySpark will transform a Spark DataFrame into a new DataFrame without altering the original data. This means that Spark is **immutable** (i.e there is no `inplace=True` argument like some `pandas` methods).\n",
    "\n",
    "For example, operations will return transformed results rather than mutating the original. Therefore, it is quite common to see:\n",
    "```python\n",
    "sdf = sdf.withColumn(\n",
    "    'int_col',\n",
    "    F.col('str_numerical_col').cast('INT')\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## Lazy Evaluation at the High Level\n",
    "Finally, Spark operations are evaluated lazily. This is because there is a driver under-the-hood which looks to optimize and make your operations more efficient. This means that your data does not \"move\" until called upon. Let's explore how this works at the high-level.\n",
    "\n",
    "### DAG (Directed Acyclic Graph)\n",
    "For those who have not taken 2nd year algorithms, a DAG is a graph where there are \"no loops\". That is, one can only traverse forward and never backwards. This is especially useful (in this context) with representing a job or processes. For example:\n",
    "\n",
    "![dag](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fmichal.karzynski.pl%2Fimages%2Fillustrations%2F2017-03-19%2Fairflow-example-dag.png&f=1&nofb=1)\n",
    "\n",
    "Here, we have a DAG generated by Apache Airflow which manages a certain job. From left to right, things are run in order.\n",
    "\n",
    "### Spark, DAGs, and Lazy Evaluation \n",
    "Specifically for Spark, transformations are added to a DAG similar to the above example. Whenever the Spark driver requests data (i.e writing, using `.collect()`, etc), the DAG gets executed. This means that until we need to **physicalize** the data, no code is run at all.\n",
    "\n",
    "Why is this good? The major advantage is that Spark makes several code optimizations under-the-hood by looking at the whole DAG each time you add a step. For any in-built transformation or function, Spark will optimize the efficiency by reordering the steps or taking shortcuts to simplify several steps. This is not possible if the data is physicalized each time you run it like `pandas`. \n",
    "\n",
    "\n",
    "Example:\n",
    "- You execute every transformation like `pandas`.\n",
    "- This means you must physicalise every intermediate step into memory. Whilst this can be fast, you are limited by the memory (RAM). \n",
    "- When you take into account the costs of RAM over time (especially if this is an ETL pipeline that you run daily), this is not efficient. This is because you are never really interested in the intermediate transformation steps when running a production pipeline. \n",
    "- As such, your job when writing Spark is to tell Spark the overall process and let it figure it out.\n",
    "\n",
    "General Rules:\n",
    "- Always use built-in functions and methods where possible. This is because Spark is designed to optimize code that it understands.\n",
    "- When applying a custom user function known as a **User Defined Function** (UDF), Spark does not understand the function code. This means it is is black box and will eat up resources and time.\n",
    "- Never use `.collect()` if possible, aim to plan your code out and only physicalize your data when required. \n",
    "- Save \"checkpoints\" for your dataset. If you have applied a few transformations and are happy with this intermediate step, save it. That way, you don't need to keep rerunning your code every time you need the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Fields and Data Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:22.546185Z",
     "start_time": "2022-07-18T07:53:18.867021Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:22.572781Z",
     "start_time": "2022-07-18T07:53:22.549842Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions:\n",
    "```python\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "\n",
    "# example 2 for applying UDFs\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    some_udf(F.col('column_from'))\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:22.623952Z",
     "start_time": "2022-07-18T07:53:22.574906Z"
    }
   },
   "outputs": [],
   "source": [
    "# example of renaming a column (we won't save it)\n",
    "sdf.withColumnRenamed(\n",
    "    'VendorID',\n",
    "    'vendor_id'\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:22.762967Z",
     "start_time": "2022-07-18T07:53:22.625231Z"
    }
   },
   "outputs": [],
   "source": [
    "# converting a couple columns to integers and saving it\n",
    "for field in ('PU', 'DO'):\n",
    "    field = f'{field}LocationID'\n",
    "    sdf = sdf.withColumn(\n",
    "        field,\n",
    "        F.col(field).cast('INT')\n",
    "    )\n",
    "    \n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for the list of accepted data types: https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some more advanced conversions. For example, if we look at the `store_and_fwd_flag`, it actually represents a boolean condition. According to the Data Dictionary though, we currently have `N` and `Y` representing `No` and `Yes` respectively.\n",
    "\n",
    "In pandas, we would have done something like this:\n",
    "```python\n",
    "df['store_and_fwd_flag'] = (df['store_and_fwd_flag'] == 'Y').astype(bool)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.024261Z",
     "start_time": "2022-07-18T07:53:22.764251Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\n",
    "    'store_and_fwd_flag',\n",
    "    (F.col(\"store_and_fwd_flag\") == 'Y').cast('BOOLEAN')\n",
    ")\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you can also do built-in `if`/`else` based results with `F.when()` and `.otherwise()`. Let's say we want a boolean field to determine if the record is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.231516Z",
     "start_time": "2022-07-18T07:53:24.025462Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\n",
    "    'is_valid_record',\n",
    "    # when we have non-zero distance/passenger/total amount then True\n",
    "    # else False\n",
    "    F.when(\n",
    "        (F.col('trip_distance') > 0)\n",
    "        & (F.col('passenger_count') > 0)\n",
    "        & (F.col('total_amount') > 0),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:25.312818Z",
     "start_time": "2022-07-18T07:53:24.232850Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to read through the data dictionary carefully to determine which records are valid or invalid. As long as you justify your logic (and it adheres to the data dictionary), then you will get marks.\n",
    "\n",
    "Be especially careful with `total_amount` as it is pretty much the addition of several other fields making it a useless feature when conducting analysis or using it in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "For those who have taken database systems or prefer using SQL, you can use Spark SQL to run queries.\n",
    "\n",
    "Whilst there are plenty of options (creating tables, views, temp tables, etc), we'll stick with views. If you are unsure what a view is, think of it as some kind of layer that sits on top of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:29.536174Z",
     "start_time": "2022-07-18T07:53:25.317561Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a temporary SQL view for the DataFrame\n",
    "sdf.createOrReplaceTempView('taxi')\n",
    "\n",
    "sql_query = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM taxi\n",
    "WHERE passenger_count == 5\n",
    "    AND trip_distance > 0\n",
    "\"\"\")\n",
    "\n",
    "sql_query.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoPandas\n",
    "- **NOTE: This only applies on the more recent datasets that use zones over coordinates**\n",
    "\n",
    "Requirements:\n",
    "- `geopandas`\n",
    "\n",
    "Shapefile Links:\n",
    "- https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip\n",
    "- https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
    "\n",
    "**Installation (MacOS Intel chip, Linux, WSL/WSL2):**\n",
    "- MacOS and Linux users use `pip3 install geopandas` or equivalent.\n",
    "\n",
    "**Installation (MacOS M1/M2 chip):**\n",
    "```bash\n",
    "brew install geos\n",
    "export DYLD_LIBRARY_PATH=/opt/homebrew/opt/geos/lib/\n",
    "pip3 install shapely\n",
    "brew install gdal\n",
    "pip3 install fiona\n",
    "brew install proj\n",
    "pip3 install pyproj\n",
    "pip3 install pygeos\n",
    "pip3 install geopandas\n",
    "```\n",
    "If you run into any errors related to `pyproj` please restart your terminal. It should work after.\n",
    "\n",
    "Why do I need to do this for MacOS M1/M2 chip? `geopandas` itself is written in Python so there are no issues, but it depends on other libraries that are written in `C/C++` and need to be compiled. The Intel and M1/M2 architecture is difference, hence this roundabout method of installing.\n",
    "\n",
    "**Installation (Windows 10/11):**\n",
    "1. Visit https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "2. You will need 2 different `.whl` (wheel) files. These are `GDAL` and `fiona`.\n",
    "    - `GDAL`: https://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal\n",
    "    - `fiona`: https://www.lfd.uci.edu/~gohlke/pythonlibs/#fiona\n",
    "3. Download the version **corresponding to your device OS and Python**. For example:\n",
    "    - `Fiona‑1.8.21‑cp311‑cp311‑win_amd64.whl` represents the `fiona v1.8.21` wheel designed for `c-python 3.11` for `windows` running a `64bit` architecture (`amd64`).\n",
    "4. Once both packages are downloaded, you will need to open up command prompt and `cd` into the directory. \n",
    "5. Install the dependencies **in this specific order**. \n",
    "    - `GDAL`  (wheel you downloaded)\n",
    "    - `fiona` (wheel you downloaded)\n",
    "    - `geopandas` (`pip` package)\n",
    "    \n",
    "Example for Windows 11 (64 bit) running `Python 3.9.X`:\n",
    "```bash\n",
    "# cd into directory containing files\n",
    "cd geopandas_dependencies\n",
    "pip3 install GDAL‑3.4.3‑cp39‑cp39‑win_amd64.whl\n",
    "pip3 install Fiona‑1.8.21‑cp39‑cp39‑win_amd64.whl\n",
    "pip3 install geopandas\n",
    "```\n",
    "\n",
    "Why do I need to do this for Windows? Like many other useful Data Science and Engineering packages, they are designed to be native to Linux and bash. Windows OS is not suitable hence our recommendation to install WSL/WSL2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:29.715326Z",
     "start_time": "2022-07-18T07:53:29.538273Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your OS and Python version, you may recieve a warning with `pygeos`. As long as it works, it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:29.837957Z",
     "start_time": "2022-07-18T07:53:29.716773Z"
    }
   },
   "outputs": [],
   "source": [
    "# sf stands for shape file\n",
    "sf = gpd.read_file(\"../../data/taxi_zones/taxi_zones.shp\")\n",
    "zones = pd.read_csv(\"../../data/taxi_zones/taxi+_zone_lookup.csv\")\n",
    "\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapefiles are one way of storing geometric objects. When working with these, you will usually need to convert it into a more human-understandable coordinate system such as latitude/longitude. See https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/understand-epsg-wkt-and-other-crs-definition-file-types/ for details on conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:29.902477Z",
     "start_time": "2022-07-18T07:53:29.838945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the geometry shaape to to latitude and longitude\n",
    "# Please attribute this if you are using it\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the `geometry` attribute now looks a bit more familiar and understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:29.907498Z",
     "start_time": "2022-07-18T07:53:29.903545Z"
    }
   },
   "outputs": [],
   "source": [
    "zones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which attribute should we join on? What kind of join will we do i.e left, right, inner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:29.922992Z",
     "start_time": "2022-07-18T07:53:29.908628Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.merge(zones, sf, on='LocationID', how='inner')\n",
    ")\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We can now combine `geopandas` with `folium` to plot nice zones. Requirements:\n",
    "- A `folium` map object with a central coordinate of interest\n",
    "- A `GeoJSON` that can be parsed by `folium`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GeoJSON` should be a `JSON` object representing a specific zone (`LocationID`) with its geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:30.041787Z",
     "start_time": "2022-07-18T07:53:29.923873Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a JSON \n",
    "geoJSON = gdf[['LocationID', 'geometry']].drop_duplicates('LocationID').to_json()\n",
    "\n",
    "# print the first 300 chars of the json\n",
    "print(geoJSON[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add it to the `folium` map object. We'll use what we call a **Choropleth Map** to show zones.\n",
    "\n",
    "Folium Docs: http://python-visualization.github.io/folium/modules.html?highlight=choropleth#folium.features.Choropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:30.158875Z",
     "start_time": "2022-07-18T07:53:30.042942Z"
    }
   },
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:30.926118Z",
     "start_time": "2022-07-18T07:53:30.159843Z"
    }
   },
   "outputs": [],
   "source": [
    "_map = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on how to plot aggregated data.\n",
    "_map.add_child(folium.Choropleth(\n",
    "    geo_data=geoJSON,\n",
    "    name='choropleth',\n",
    "))\n",
    "\n",
    "_map.save('../../plots/foliumChoroplethMap.html')\n",
    "_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Checklist for Visualisations and Dashboards:\n",
    "1. Your visualisation needs to tell a story.\n",
    "2. It should be interpretable without being overly verbose.\n",
    "3. The scale and axis need to make sense (and you can assume the reader knows the difference between a normal scale vs log scale).\n",
    "4. The choice of visualisation needs to make sense:\n",
    "    - Line plot vs Bar chart with non-numerical categories\n",
    "    - Map plot with points vs clusters for each location\n",
    "    - Scatterplot vs Histogram plot to see distribution\n",
    "    - etc\n",
    "5. Choice of colour scheme / alpha / size need to be easy on the eyes.\n",
    "\n",
    "At the end of the day, even if you think your visualisation is \"pretty\" or \"beautiful\", if a reader cannot understand it, then it is not a good visualisation.\n",
    "\n",
    "Let's go through an example on **pickup locations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:30.955561Z",
     "start_time": "2022-07-18T07:53:30.927766Z"
    }
   },
   "outputs": [],
   "source": [
    "# first off, join the geometries with the dataset\n",
    "df = pd.read_parquet('../../data/tute_data/sample_data.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:31.021877Z",
     "start_time": "2022-07-18T07:53:30.956584Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .merge(gdf[['LocationID', 'geometry']], left_on='PULocationID', right_on='LocationID') \\\n",
    "    .drop('LocationID', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, many students may sum each zone (location) and plot the total earnings to gain a rough idea of income. Whilst this can be okay for a first analysis, it may be more useful to look at proportion by dividing by frequency.\n",
    "\n",
    "Why is this a more suitable idea?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:31.037986Z",
     "start_time": "2022-07-18T07:53:31.023323Z"
    }
   },
   "outputs": [],
   "source": [
    "proportions = df[['PULocationID', 'total_amount']] \\\n",
    "                .groupby('PULocationID') \\\n",
    "                .agg(\n",
    "                    {\n",
    "                        'total_amount': 'sum', # sum over total amount earned\n",
    "                        'PULocationID': 'count' # count number of instances from sample\n",
    "                    }\n",
    "                ) \\\n",
    "                .rename({'PULocationID': 'total_trips'}, axis=1)\n",
    "\n",
    "proportions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:31.046234Z",
     "start_time": "2022-07-18T07:53:31.039466Z"
    }
   },
   "outputs": [],
   "source": [
    "proportions['avg_trip_amount'] = proportions['total_amount'] / proportions['total_trips']\n",
    "proportions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, this is **only a random sample of 5% of the true population**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:31.759237Z",
     "start_time": "2022-07-18T07:53:31.047401Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.73, -73.74], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "c = folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=df, # data source\n",
    "    columns=['PULocationID','total_amount'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='YlOrRd', # color scheme\n",
    "    nan_fill_color='black',\n",
    "    legend_name='Average Trip Earnings USD$'\n",
    ")\n",
    "\n",
    "c.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do the black coloured zones represent?\n",
    "- Which area seems most profitable so far?\n",
    "\n",
    "Let's add some markers for the airports to differentiate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:31.772253Z",
     "start_time": "2022-07-18T07:53:31.760361Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf.loc[gdf['Zone'].str.contains('Airport')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `folium` [Marker object](https://python-visualization.github.io/folium/modules.html?highlight=marker#folium.map.Marker) requires coordinates. \n",
    "\n",
    "We can derive zone centroids using `geopandas`'s `.centroid` attribute of a `geometry`. This will return a `Point` object (not what we want just yet) which can be converted into `latitude` (`y`) and `longitude` (`x`) coordinates by accessing the `.x` and `.y` attribute of the `Point`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:31.797909Z",
     "start_time": "2022-07-18T07:53:31.773315Z"
    }
   },
   "outputs": [],
   "source": [
    "# (y, x) since we want (lat, long)\n",
    "gdf['centroid'] = gdf['geometry'].apply(lambda x: (x.centroid.y, x.centroid.x))\n",
    "gdf[['Zone', 'LocationID', 'centroid']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:32.199064Z",
     "start_time": "2022-07-18T07:53:31.799113Z"
    }
   },
   "outputs": [],
   "source": [
    "for zone_name, coord in gdf.loc[gdf['Zone'].str.contains('Airport'), ['Zone', 'centroid']].values:\n",
    "    m.add_child(\n",
    "        folium.Marker(location=coord, popup=zone_name)\n",
    "    )\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark User Defined Functions (UDF)\n",
    "So far, all the functions covered have been about simple aggregations, filtering rows, or changing data types. Sometimes though, you will require more advanced preprocessing techniques that are not built-in with Spark.\n",
    "\n",
    "The best way to create a UDF is to use the class decorator. Here's the general syntax:\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import SomeDataType\n",
    "\n",
    "# class decorator method for creating a UDF\n",
    "# You must always specify the expected return type (i.e string, array, int, etc)\n",
    "@F.udf(SomeDataType)\n",
    "def some_udf(col):\n",
    "    ...\n",
    "    return ...\n",
    "       \n",
    "sdf = sdf.withColumn(\n",
    "    'transformed_col',\n",
    "    some_udf(F.col('raw_col'))\n",
    ")\n",
    "```\n",
    "\n",
    "Let's grab the centroids using a Spark UDF. We cover it here as this is an example of something that Spark can't do with built-in methods.\n",
    "\n",
    "First off, we will need to convert `gdf` into a spark dataframe:\n",
    "1. Convert the `geometry` into `wkt` (Well Known Text). This is essentially a way of encoding geometry objects into text because Spark does not handle custom data types. We'll use `shapely` (a `geopandas` dependency) to do this.\n",
    "2. Create a Spark dataframe from the `gdf`.\n",
    "3. Create UDF and apply.\n",
    "\n",
    "You should now be able to join your Spark dataset with this Spark GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:13.541575Z",
     "start_time": "2022-07-18T07:55:13.470960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# attempt at creating spark df without wkt conversion\n",
    "spark.createDataFrame(\n",
    "    gdf[['Zone', 'LocationID', 'geometry']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:17.832623Z",
     "start_time": "2022-07-18T07:55:17.797832Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf['wkt'] = gdf['geometry'].to_wkt()\n",
    "gdf[['Zone', 'LocationID', 'geometry', 'wkt']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:18.897987Z",
     "start_time": "2022-07-18T07:55:17.997831Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_gdf = spark.createDataFrame(\n",
    "    gdf[['Zone', 'LocationID', 'wkt']]\n",
    ")\n",
    "\n",
    "spark_gdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:40.788821Z",
     "start_time": "2022-07-18T07:55:40.778751Z"
    }
   },
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "@F.udf(ArrayType(FloatType()))\n",
    "def get_centroids(wkt_geo):\n",
    "    centroid = wkt.loads(wkt_geo).centroid\n",
    "    return centroid.y, centroid.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:42.229402Z",
     "start_time": "2022-07-18T07:55:41.268549Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_gdf = spark_gdf.withColumn(\n",
    "    'geometry',\n",
    "    get_centroids(F.col('wkt'))\n",
    ")\n",
    "\n",
    "spark_gdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:42.233305Z",
     "start_time": "2022-07-18T07:55:42.230451Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Visualizations\n",
    "We recommend that you plot and look at these attributes in your own time using `matplotlib` and `seaborn`:\n",
    "- Scatterplot of `fare_amount` vs `trip_amount`. \n",
    "    -What's the relationship like? \n",
    "    -Why are there many values around 0?\n",
    "- Histogram and distribution plot of `fare_amount`, `trip_amount`, `trip_distance`. \n",
    "    - Is the distribution skewed? \n",
    "    - Does a log transformation make the distribution nicer? \n",
    "    - What outliers do we have?\n",
    "- Correlation Heatmap between attributes of relevance.\n",
    "    - Which attributes should we choose?\n",
    "    - Does correlation imply causality?\n",
    "    \n",
    "You may also apply relevant transformations where suitable i.e `log`. Just make sure you **state it clearly** in your figure caption or legend.\n",
    "\n",
    "A revision of skewness (in case you have forgotten):\n",
    "![skew](https://mammothmemory.net/images/user/base/Maths/Statistics%20and%20probability/Standard%20deviation/skewed-distribution-graphs.c97bc76.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:43.378060Z",
     "start_time": "2022-07-18T07:55:42.860494Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:43.630995Z",
     "start_time": "2022-07-18T07:55:43.379040Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())\n",
    "# wow that's easy...\n",
    "\n",
    "plt.title('Pearson Correlation Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to take note of:\n",
    "- `trip_distance` highly correlates with high tips, tolls and overall trip amount\n",
    "- `payment_type` seems to have some form of negative correlation with `tip_amount`. **Be careful as this is a discrete category.**\n",
    "- Having `VendorID` as an feature **is misleading**, why??? \n",
    "\n",
    "**Important:** Only include numerical and ordinal features when computing the Pearson Correlation metric. You cannot compute the correlation between a category and numerical feature (i.e `VendorID` vs `payment_type` vs `trip_distance`).\n",
    "\n",
    "How about Locations? Does correlation work for it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:44.102628Z",
     "start_time": "2022-07-18T07:55:43.951031Z"
    }
   },
   "outputs": [],
   "source": [
    "CORR_COLS = [\n",
    "    \"passenger_count\", \"trip_distance\", \"fare_amount\", \"extra\", \n",
    "    \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \n",
    "    \"total_amount\", \"airport_fee\"\n",
    "]\n",
    "\n",
    "sns.heatmap(df[CORR_COLS].corr())\n",
    "\n",
    "plt.title('Pearson Correlation Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you're interested in calculating correlation between nominal and continuous data, here's a [great explanation](https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618).   \n",
    "- Remember, you need to refer back to the data dictionary as well as the fare page: https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page\n",
    "\n",
    "- You should especially take note of the fare page if you're looking to see how `RatecodeID` plays a role on the fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to use `pyspark` for the full dataset, you can compute the Pearson correlation between any two features. We don't expect students to use the full distribution for each field or use Spearman for nominal features, though, you are more than welcome to do so if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:55:46.639846Z",
     "start_time": "2022-07-18T07:55:45.016981Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "sdf \\\n",
    "    .where(\n",
    "        F.col('payment_type') == 1\n",
    "    ) \\\n",
    "    .corr('trip_distance', 'tip_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cover this in a bit more detail in the next tutorial, though, just be aware that `pyspark.ml` is out of scope for this subject due to time constraints.\n",
    "\n",
    "`VectorAssembler` is a function used to merge multiple columns into a single vector column. This is required as Spark works for single vectors only. Just note that Spark's correlation is still a Work-In-Progress (WIP) so it can't handle `NULL` values. Make sure to drop or impute them (with justification) for your own project.\n",
    "\n",
    "The process for this mirrors `sklearn` i.e `model.fit()` or `data.transform()` so this shouldn't be too unfamiliar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:02.546462Z",
     "start_time": "2022-07-18T07:55:48.351148Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "features = \"correlation_features\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=CORR_COLS, # input names (can be list of fields)\n",
    "    outputCol=features # output name (single vector output)\n",
    ")\n",
    "\n",
    "# transform the features -> this is similar to sklearn's .fit() or .transform()\n",
    "feature_vector = assembler \\\n",
    "                .transform(\n",
    "                    sdf.dropna('any')\n",
    "                ) \\\n",
    "                .select(features)\n",
    "\n",
    "corr_matrix_dense = Correlation.corr(feature_vector, features)\n",
    "corr_matrix_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this intermediate step, we have a dense vector output. This is one exception where using `.collect()` is required. We'll grab the dense vector and create  a `pandas` dataframe. There's no need for `pyspark` since the results are computed already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:02.596128Z",
     "start_time": "2022-07-18T07:56:02.547485Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_matrix = corr_matrix_dense.collect()[0][0].toArray().tolist()\n",
    "\n",
    "df_corr = pd.DataFrame(corr_matrix, index=CORR_COLS, columns=CORR_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:02.602345Z",
     "start_time": "2022-07-18T07:56:02.596919Z"
    }
   },
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst the precision is excellent (and this is why using Spark is great over pandas), when presenting a report there should be at most 4 decimal places. \n",
    "\n",
    "We'll round it to 4 decimal places using a `pandas` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:02.605125Z",
     "start_time": "2022-07-18T07:56:02.603600Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format # any number of digits with 2 floating points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:02.609934Z",
     "start_time": "2022-07-18T07:56:02.605768Z"
    }
   },
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the differences in correlation between the full distribution and 5% random sample.\n",
    "\n",
    "What we do below **is not a valid comparison**. We are merely using it tell you that the LHS is not the same as the RHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:02.641542Z",
     "start_time": "2022-07-18T07:56:02.611192Z"
    }
   },
   "outputs": [],
   "source": [
    "df[CORR_COLS].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:02.672476Z",
     "start_time": "2022-07-18T07:56:02.642292Z"
    }
   },
   "outputs": [],
   "source": [
    "abs(df_corr.abs() - df[CORR_COLS].corr().abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this... there is a significant difference between `correlation(total_amount, tolls_amount)` between the two correlation matrices. **Be careful**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering?\n",
    "- We want to see if the the profitability of zones remains consistent with respect to hour of day, day of week and pickup location. The distribution of profitable zones should be similar across all years.\n",
    "\n",
    "- How is a zone profitable? Frequency of trips? Duration of trips? Best \"earners\"? We've had creative metrics over the past few years that students invented. \n",
    "\n",
    "- For example, you could create your own feature and scale it accordingly. Perhaps the expected dollar per minute + possible tolls scaled by the expected frequency of trips might be a good start.\n",
    "\n",
    "- Just remember that trip frequency $\\approx$ taxi demand in a zone (you don't know the true number of taxis in a zone at the time).\n",
    "\n",
    "- Additionally, variable rate fares exist: _\"50 cents per 1/5 mile when travelling above 12mph OR 50 cents per 60 seconds in slow traffic or when the vehicle is stopped.\"_ \n",
    "\n",
    "- This means profit rates may require you to state the assumption that you are assuming constant velocity throughout the trip. We have had students in the past approximate this by finding speed limits of zones in NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
