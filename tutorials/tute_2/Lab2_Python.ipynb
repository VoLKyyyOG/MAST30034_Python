{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you **Restart and Clear Output** the notebook kernel so there is sufficient memory for below.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/tute_data/sample_data.csv')\n",
    "\n",
    "## Plotting a Geospatial map using `folium` and `geopandas`\n",
    "You'll need to `pip3 install folium geopandas` for this section.\n",
    "\n",
    "Some terminology before we continue:\n",
    "- Map Tiles / Tile Providers: The underlying map \"style\" (i.e Google Maps vs Open Street View)\n",
    "\n",
    "Let's start off with `folium`.\n",
    "\n",
    "import folium\n",
    "\n",
    "# initialise a map\n",
    "_map = folium.Map(tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# save map\n",
    "_map.save('../../plots/map.html')\n",
    "\n",
    "# show map\n",
    "_map\n",
    "\n",
    "Since `folium` generates interactive maps, you will need to save static versions for your report.\n",
    "\n",
    "The easiest way is to open the saved `html` plot and take a **good and high-quality screenshot**. Make sure to take into account the title, legend, and font size.\n",
    "\n",
    "## Where to go from here\n",
    "We've "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Inferences\n",
    "- More pickups around central Manhattan, with more dropoffs in the surrounding bouroughs.\n",
    "- Pickup location are easily divided into \"hubs\" (i.e Manhattan, Aiport, etc).\n",
    "- Dropoffs seem to be scattered across the map.\n",
    "\n",
    "**IMPORTANT:** The above is at most *describing* the plot. Your project will require *analysis* and *research* on top of describing a plot. That is:\n",
    "- *Why might there be more pickups around central Manhattan?*\n",
    "- *Is there an explanation surrounding the \"hubs\"?*\n",
    "- *Why are dropoffs scattered across the map?*\n",
    "\n",
    "As a suggestion, have less description and more analysis. Your visualisation should ensure that it can be easily interpreted and visible (i.e suitable font size, colour, alpha, legend, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Overview\n",
    "## First Half\n",
    "More Visualisations:\n",
    "- Map Clusters\n",
    "- Heatmaps (GIS)\n",
    "- HexBins vs SquareBins \n",
    "- Choropleths\n",
    "\n",
    "Descriptive Statistics:\n",
    "- Histograms and looking at the distribution shape.\n",
    "- How to \"quick plot\" dataframes using `pandas`.\n",
    "- Some methods of determining bin sizes.\n",
    "\n",
    "## Second Half\n",
    "Revision:\n",
    "- Any code related questions for Python\n",
    "- (Windows 10 Users) Installing WSL2 (Ubuntu 20.04) for a clean environment.\n",
    "\n",
    "Advanced Content:\n",
    "- Introduction to `PySpark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations\n",
    "- Clustering points\n",
    "- Binned plots\n",
    "- Good visualizations vs Bad visualisations\n",
    "\n",
    "\n",
    "## Personal Checklist for Visualisations and Dashboards:\n",
    "1. Your visualisation needs to tell a story.\n",
    "2. It should be interpretable without being overly verbose.\n",
    "3. The scale and axis need to make sense (and you can assume the reader knows the difference between a normal scale vs log scale).\n",
    "4. The choice of visualisation needs to make sense:\n",
    "    - Line plot vs Bar chart with non-numerical categories\n",
    "    - Map plot with points vs clusters for each location\n",
    "    - Scatterplot vs Histogram plot to see distribution\n",
    "    - etc\n",
    "5. Choice of colour scheme / alpha / size need to be easy on the eyes.\n",
    "\n",
    "At the end of the day, even if you think your visualisation is \"pretty\" or \"beautiful\", if a reader cannot understand it, then it is not a good visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:33:21.044872Z",
     "start_time": "2021-08-03T04:33:20.854978Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:33:24.756283Z",
     "start_time": "2021-08-03T04:33:24.603730Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_feather(\"../data/lab_specific/sample.feather\").drop('index', axis=1)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "- Excellent tool when visualising geospatial coordinates\n",
    "- Aggregates certain \"areas\" to make distributions far easier to interpret\n",
    "\n",
    "Questions:\n",
    "- *How is this worse/better than the previous scatter plot?*\n",
    "- *Does this plot give us any useful information?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:34:11.442719Z",
     "start_time": "2021-08-03T04:34:09.667709Z"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import FastMarkerCluster\n",
    "\n",
    "# lat, long\n",
    "COORDS = ['pickup_latitude', 'pickup_longitude']\n",
    "\n",
    "# create an interactive geospatial graph\n",
    "pickups_cluster = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# use a built-in clustering algorithm to apply markers for hotspots\n",
    "pickups_cluster.add_child(FastMarkerCluster(data=df[COORDS].values))\n",
    "\n",
    "# visualize the plot \n",
    "pickups_cluster.save('../plots/foliumFastCluster.html')\n",
    "pickups_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps\n",
    "- Similar to clustering with respect to visualising geospatial coordinates.\n",
    "- Allows for a more continuous interpretation of the map, rather than clusters.\n",
    "\n",
    "Questions:\n",
    "- *In terms of pickups, why might we prefer or not prefer this over a cluster map?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:40:12.172552Z",
     "start_time": "2021-08-03T04:40:10.800861Z"
    }
   },
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "pickups_heatmap = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "pickups_heatmap.add_child(HeatMap(df[COORDS].values, radius=10))\n",
    "\n",
    "pickups_heatmap.save('../plots/foliumPickupHeatmap.html')\n",
    "pickups_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:40:08.599651Z",
     "start_time": "2021-08-03T04:40:07.301750Z"
    }
   },
   "outputs": [],
   "source": [
    "dropoffs_heatmap = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "dropoffs_heatmap.add_child(HeatMap(df[['dropoff_latitude', 'dropoff_longitude']].values, radius=10))\n",
    "\n",
    "dropoffs_heatmap.save('../plots/foliumDropoffHeatmap.html')\n",
    "dropoffs_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hex Binned Plots\n",
    "- An extension of heatmaps that discretises the points.\n",
    "- Gives a greater granularity to the map when looking at geospatial coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:40:33.842594Z",
     "start_time": "2021-08-03T04:40:33.600798Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.tile_providers import get_provider, Vendors\n",
    "from bokeh.io import save, reset_output, output_notebook\n",
    "\n",
    "TILE = get_provider(\"STAMEN_TERRAIN_RETINA\")\n",
    "\n",
    "reset_output()\n",
    "output_notebook()\n",
    "# note below that it says \"BokehJS 1.4.0 successfully loaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:40:34.844290Z",
     "start_time": "2021-08-03T04:40:34.826505Z"
    }
   },
   "outputs": [],
   "source": [
    "df[COORDS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:40:36.150721Z",
     "start_time": "2021-08-03T04:40:35.871974Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "# create left and right boundaries for the x and y axis respectively\n",
    "xRange = [-74, -73.9]\n",
    "yRange = [40.5, 40.9]\n",
    "\n",
    "def lat2mercer(coords):\n",
    "    k = 6378137\n",
    "    converted = list()\n",
    "    for lat in coords:\n",
    "        converted.append(np.log(np.tan((90 + lat) * np.pi/360.0)) * k)\n",
    "    return converted\n",
    "\n",
    "def lon2mercer(coords):\n",
    "    k = 6378137\n",
    "    converted = list()\n",
    "    for lon in coords:\n",
    "        converted.append(lon * (k * np.pi/180.0))\n",
    "    return converted\n",
    "\n",
    "# convert to mercer\n",
    "df['pickupX'] = df['pickup_longitude'].apply(lambda x: lon2mercer([x])[0])\n",
    "df['pickupY'] = df['pickup_latitude'].apply(lambda x: lat2mercer([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:40:41.958019Z",
     "start_time": "2021-08-03T04:40:41.886107Z"
    }
   },
   "outputs": [],
   "source": [
    "# create bokeh figure, where x_range and y_range are in mercer\n",
    "p = figure(x_range=lon2mercer(xRange), y_range=lat2mercer(yRange),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
    "# add map tile\n",
    "p.add_tile(TILE)\n",
    "# change title\n",
    "p.title.text = \"Hex-Binned Pickups in NYC\"\n",
    "\n",
    "# add hexbin (size param is the bin size - more about it next week)\n",
    "p.hexbin(x=df['pickupX'], y=df['pickupY'], size=250)\n",
    "\n",
    "show(p)\n",
    "save(p, '../plots/bokehPickupHexBinned.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the color and adding color bar for a better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColorBar, LinearColorMapper\n",
    "from bokeh.palettes import all_palettes\n",
    "\n",
    "# create bokeh figure, where x_range and y_range are in mercer\n",
    "p = figure(x_range=lon2mercer(xRange), y_range=lat2mercer(yRange),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
    "# add map tile\n",
    "p.add_tile(TILE)\n",
    "# change title\n",
    "p.title.text = \"Hex-Binned Pickups in NYC\"\n",
    "\n",
    "palette = all_palettes['Magma'][256][::-1]\n",
    "color_mapper = LinearColorMapper(palette=palette, low=1, high=1449)\n",
    "color_bar = ColorBar(color_mapper=color_mapper, label_standoff=12)\n",
    "r, bins = p.hexbin(x=df['pickupX'], y=df['pickupY'], size=250, palette=palette)\n",
    "\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "show(p)\n",
    "save(p, '../plots/bokehPickupHexBinned.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative using matplotlib. Although fast, it's not useful without the underlying geospatial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:42:18.702382Z",
     "start_time": "2021-08-03T04:42:18.393257Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test = df.dropna()\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.hexbin(x = test[COORDS[1]], y= test[COORDS[0]],\n",
    "           gridsize=100, bins='log', cmap='inferno')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapefile Overlays\n",
    "- **NOTE: This only applies on the more recent datasets that use zones over coordinates**\n",
    "\n",
    "Requirements:\n",
    "- `geopandas`\n",
    "\n",
    "Shapefile Links:\n",
    "- https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip\n",
    "- https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
    "\n",
    "Installation:\n",
    "- MacOS and Linux users use `pip install geopandas` or equivalent.\n",
    "- Windows users may need to visit another guide if there are issues (see manual installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual:\n",
    "1. Visit https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "2. You will need 2 different `.whl` (wheel) files. These are `GDAL` and `Fiona`.\n",
    "3. Find both of them and download the version **corresponding to your device OS and Python**. For example, a 64-bit Windows 10 device running Python 3.8.X will need to download this file for `GDAL`. Repeat this for `Fiona` as well. (For your convenience, a fresh installation of Ubuntu 20.04 will yield Python 3.8.3, and most modern devices all run on 64-bit architecture. I have provided these in the `geopandas_dependencies` folder)\n",
    "\n",
    "![gdal](./geopandas_dependencies/tute.PNG)\n",
    "\n",
    "4. Once they are downloaded, you will need to open up command prompt and `cd` into the directory. For example, if my `.whl` files are in the `geopandas_dependencies` folder, the command I would use is:\n",
    "    - `cd C:\\Users\\USERNAME\\Documents\\GitHub\\MAST30034Repo\\workbook\\geopandas_dependencies`\n",
    "5. Install the dependencies in this order:\n",
    "    - `pip install GDAL-3.1.2-cp37-cp37m-win_amd64.whl` (or the corresponding file you downloaded)\n",
    "    - `pip install Fiona-1.8.13-cp37-cp37m-win_amd64.whl`\n",
    "6. Run `pip install geopandas` and it should now work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:43:32.319486Z",
     "start_time": "2021-08-03T04:43:32.296442Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/lab_specific/yellow_tripdata_2019-01.csv\")\n",
    "df.groupby('PULocationID')['payment_type'].count().reset_index().sort_values('payment_type', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:45:01.340157Z",
     "start_time": "2021-08-03T04:45:01.078241Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# sf stands for shape file\n",
    "sf = gpd.read_file(\"../data/taxi_zones/taxi_zones.shp\")\n",
    "zone = pd.read_csv(\"../data/taxi_zones/taxi+_zone_lookup.csv\")\n",
    "\n",
    "# Convert the geometry shaape to to latitude and longitude\n",
    "# Please attribute this if you are using it\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:45:02.007128Z",
     "start_time": "2021-08-03T04:45:01.915879Z"
    }
   },
   "outputs": [],
   "source": [
    "sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Conventions:\n",
    "- `gdf` stands for GeoDataFrame.\n",
    "- Now, we need to create a `gdf` with our current dataframe and shapefile data.\n",
    "\n",
    "From `df`, we join using `PULocationID` to match from `sf`'s `LocationID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:46:13.881038Z",
     "start_time": "2021-08-03T04:46:13.847436Z"
    }
   },
   "outputs": [],
   "source": [
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:50:07.869562Z",
     "start_time": "2021-08-03T04:50:07.774001Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(df, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)\n",
    "gdf.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try plot this with `folium`. There are two ways to approach this kind of task:\n",
    "- Plot all the data!\n",
    "- Aggregate *before* plotting data!\n",
    "\n",
    "It's up to you to decide which to use...\n",
    "\n",
    "Specifically with `folium`, you will need to create a GeoJSON format. You can view more information using the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:54:51.616540Z",
     "start_time": "2021-08-03T04:54:51.533632Z"
    }
   },
   "outputs": [],
   "source": [
    "geoJSON = gdf[['LocationID','geometry']].drop_duplicates('LocationID').to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is just shapefiles based on the locations and **does not** show any analysis.\n",
    "\n",
    "Use this as a reference on how you can plot shapefiles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:55:33.930995Z",
     "start_time": "2021-08-03T04:55:33.390089Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on how to plot aggregated data.\n",
    "m.add_child(folium.Choropleth(\n",
    "    geo_data=geoJSON,\n",
    "    name='choropleth',\n",
    "))\n",
    "\n",
    "m.save('../plots/foliumChoroplethMap.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:56:33.165229Z",
     "start_time": "2021-08-03T04:56:32.636387Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# an example of what the geoJSON looks like\n",
    "json.loads(geoJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:02:17.113163Z",
     "start_time": "2021-08-03T05:02:17.041347Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf.loc[gdf['total_amount'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:59:52.974090Z",
     "start_time": "2021-08-03T04:59:52.964654Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf[['LocationID','total_amount']].groupby('LocationID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:00:34.217459Z",
     "start_time": "2021-08-03T05:00:33.623118Z"
    }
   },
   "outputs": [],
   "source": [
    "m_trip_distance = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=gdf, # data source\n",
    "    columns=['LocationID','total_amount'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='OrRd', # color scheme\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Trips' # legend title\n",
    ").add_to(m_trip_distance)\n",
    "\n",
    "m_trip_distance.save('../plots/foliumChoroplethMapTrips.html')\n",
    "m_trip_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Descriptive Statistics\n",
    "- Hopefully nothing here is *new* to you...\n",
    "- This tutorial is more about efficiently getting there and some food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:14:11.466596Z",
     "start_time": "2021-08-03T05:14:11.249149Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:14:12.792106Z",
     "start_time": "2021-08-03T05:14:12.635291Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_feather(\"../data/lab_specific/sample.feather\").dropna().drop('index', axis=1)\n",
    "\n",
    "# describe our data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that not all the data should be interpreted as purely numerical...\n",
    "- There may be conclusions you can draw *by coincidence* if you incorrectly assume data types!\n",
    "- For example:\n",
    "    - `longitude` and `latitude` should be interpreted as geospatial coordinates\n",
    "    - `payment_type` is a discrete category of payment types\n",
    "    - `trip_distance` is non-linear (not a straight line from A to B), but we have no further data on it. It is also in **miles**.\n",
    "- To avoid misinterpreting the attributes, refer to the data dictionary provided on the TLC website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot plots for Fare / Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:16:09.240705Z",
     "start_time": "2021-08-03T05:16:09.228599Z"
    }
   },
   "outputs": [],
   "source": [
    "df['RatecodeID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:15:12.391933Z",
     "start_time": "2021-08-03T05:15:12.200185Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['fare_amount', 'trip_distance']].plot.scatter(x='fare_amount',\n",
    "                                                  y='trip_distance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Inference:\n",
    "- We can visually see that the relationship is relatively linear as you'd expect (more distance generally means more money)\n",
    "- A fair number of outliers, notably around the 0 distance axis and 0 cost axis. Why might this be the case?\n",
    "- Why are there negative values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram plots for Trip Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:17:37.036905Z",
     "start_time": "2021-08-03T05:17:36.722706Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['fare_amount'], bins=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I guess we can kind of see most fares are between 0 - 100.\n",
    "- Hard to tell where the main distribution is spread around.\n",
    "- We can enhance this using a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:18:11.526345Z",
     "start_time": "2021-08-03T05:18:11.101973Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import log, sqrt\n",
    "\n",
    "# apply a log transformation for all x non-zero x points, else 0\n",
    "def logify(x):\n",
    "    return log(x) if x else 0\n",
    "\n",
    "sns.distplot(df['fare_amount'].apply(logify), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A one-line `lambda` alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:18:13.103628Z",
     "start_time": "2021-08-03T05:18:12.670730Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['fare_amount'].apply(lambda x: log(x) if x else 0), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a log transformation to visually see the distribution\n",
    "- Now we see most the values fall under `exp(x)`, majority between `$7` - `$55` (`exp(2)` - `exp(55)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the distribution for trips under 15 miles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:19:03.399796Z",
     "start_time": "2021-08-03T05:19:03.068448Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df.loc[df['trip_distance'] <= 15, 'trip_distance']\n",
    "\n",
    "sns.distplot(data, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instant Insights:\n",
    "- Distribution is positive skewed\n",
    "- Fare distances are predominantly very short, between 1-3 miles\n",
    "- There seems to be a fare number of outliers for trips > 15 miles\n",
    "- Perhaps we should do a correlation check. Recall that we had negative fares for some reason..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:19:26.188749Z",
     "start_time": "2021-08-03T05:19:26.180925Z"
    }
   },
   "outputs": [],
   "source": [
    "# pearson (by default) correlation table for distance and fare amount\n",
    "df[['trip_distance','fare_amount']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:19:40.428089Z",
     "start_time": "2021-08-03T05:19:40.077349Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())\n",
    "# wow that's easy...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to take note of:\n",
    "- `trip_distance` highly correlates with high tips, tolls and overall trip amount\n",
    "- `payment_type` seems to have some form of negative correlation with `tip_amount`. Must be careful as this is a discrete category.\n",
    "- As you can see here, having `VendorID` as an feature is misleading, you should take any factor out when conducting Pearson Correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:20:18.785174Z",
     "start_time": "2021-08-03T05:20:18.754498Z"
    }
   },
   "outputs": [],
   "source": [
    "CORR_COLS = [\"passenger_count\", \"trip_distance\", \"fare_amount\", \"extra\", \n",
    " \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \"total_amount\"]\n",
    "\n",
    "df[CORR_COLS].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:20:31.019760Z",
     "start_time": "2021-08-03T05:20:30.795705Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df[CORR_COLS].corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you're interested in calculating correlation between nominal and continuous data, here's a [great explanation](https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618).   \n",
    "- Remember, you need to refer back to the data dictionary as well as the fare page: https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page\n",
    "\n",
    "- You should especially take note of the fare page if you're looking to see how `RatecodeID` plays a role on the fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Methods\n",
    "Sturges:  \n",
    "- $bins = ceil(log_2(n)) + 1$\n",
    "\n",
    "Rice:\n",
    "- $bins = 2\\times\\sqrt[3]{n}$\n",
    "\n",
    "Scott:\n",
    "- $bins = \\frac{max - min}{3.5\\times \\frac{SD}{\\sqrt[3]{n}}}$\n",
    "\n",
    "Freedman:\n",
    "- $bins = \\frac{max - min}{2\\times \\frac{IQR}{\\sqrt[3]{n}}}$\n",
    "\n",
    "Square:\n",
    "- $bins = \\sqrt{n}$\n",
    "\n",
    "(Source: https://www.answerminer.com/blog/binning-guide-ideal-histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:22:12.490125Z",
     "start_time": "2021-08-03T05:22:12.484167Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataframes method that may be of use\n",
    "MAX = df['fare_amount'].max()\n",
    "MIN = df['fare_amount'].min()\n",
    "SD = df['fare_amount'].std()\n",
    "IQR = df['fare_amount'].quantile()\n",
    "N = len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that implement specific binning methods. Feel free to use them if you would like so long as they are attributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:22:31.750828Z",
     "start_time": "2021-08-03T05:22:31.747434Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import log, log2\n",
    "\n",
    "def sturges(x):\n",
    "    return int(log2(x)) + 1\n",
    "\n",
    "def rice(x):\n",
    "    return int(2 * x ** (1/3))\n",
    "\n",
    "def scott(large, small, sd, x):\n",
    "    return int((large - small) / (3.5 * (sd/x ** (1/3))))\n",
    "\n",
    "def freedman(large, small, iqr, x):\n",
    "    return int((large - small) / (2 * (iqr/x ** (1/3))))\n",
    "    \n",
    "def square(x):\n",
    "    return int(sqrt(x))\n",
    "\n",
    "def logify(x):\n",
    "    return log(x) if x else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:22:37.275635Z",
     "start_time": "2021-08-03T05:22:32.889734Z"
    }
   },
   "outputs": [],
   "source": [
    "fig1 = sns.distplot(df['fare_amount'], bins=sturges(N))\n",
    "plt.title(\"Sturges Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig2 = sns.distplot(df['fare_amount'], bins=rice(N))\n",
    "plt.title(\"Rice Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig3 = sns.distplot(df['fare_amount'], bins=scott(MAX, MIN, SD, N))\n",
    "plt.title(\"Scott Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig4 = sns.distplot(df['fare_amount'], bins=freedman(MAX, MIN, IQR, N))\n",
    "plt.title(\"Freedman Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig5 = sns.distplot(df['fare_amount'], bins=square(N))\n",
    "plt.title(\"Square Binnings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T05:22:43.684850Z",
     "start_time": "2021-08-03T05:22:38.554250Z"
    }
   },
   "outputs": [],
   "source": [
    "fig1 = sns.distplot(df['fare_amount'].apply(logify), bins=sturges(N))\n",
    "plt.title(\"log Sturges Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig2 = sns.distplot(df['fare_amount'].apply(logify), bins=rice(N))\n",
    "plt.title(\"log Rice Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig3 = sns.distplot(df['fare_amount'].apply(logify), bins=scott(MAX, MIN, SD, N))\n",
    "plt.title(\"log Scott Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig4 = sns.distplot(df['fare_amount'].apply(logify), bins=freedman(MAX, MIN, IQR, N))\n",
    "plt.title(\"log Freedman Binnings\")\n",
    "plt.show()\n",
    "\n",
    "fig5 = sns.distplot(df['fare_amount'].apply(logify), bins=square(N))\n",
    "plt.title(\"log Square Binnings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rice's method for binning looks good **for this plot**\n",
    "- Not always the case, keep that in mind..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering?\n",
    "- We want to see if the the profitability of zones remains consistent with respect to hour of day, day of week and pickup location. The distribution of profitable zones should be similar across all years.\n",
    "- How is a zone profitable? Frequency of trips? Duration of trips? Best \"earners\"? \n",
    "\n",
    "- You could create your own feature and scale it accordingly. Perhaps the expected dollar per minute + possible tolls scaled by the expected frequency of trips might be a good start.\n",
    "\n",
    "- Just remember that trip frequency $\\approx$ taxi demand in a zone (you don't know the number of taxis in a zone at the time)\n",
    "\n",
    "- Additionally, variable rate fares exist: _\"50 cents per 1/5 mile when travelling above 12mph OR 50 cents per 60 seconds in slow traffic or when the vehicle is stopped.\"_\n",
    "\n",
    "- Profit rates might assume crude approximations degrading into linear distance / constant velocity / etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pre-Requisite Tasks for the Apache Spark tutorial\n",
    "\n",
    "### WSL Environment for Windows 10\n",
    "Refer to this guide to get a native Linux terminal in Windows 10:\n",
    "- https://github.com/akiratwang/COMP20003-Setting-Up\n",
    "- Ignore all the `C` related parts, just get Ubuntu installed.\n",
    "\n",
    "### Apache Spark 3.0 (PySpark) Installation\n",
    "- Visit `MAST30034/advanced_tutorials/Spark%20Installation.ipynb`\n",
    "\n",
    "## Today\n",
    "### Apache Spark 3.0 (PySpark and Spark SQL) Tutorial\n",
    "- Visit `MAST30034/advanced_tutorials/Spark%20Tutorial.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
